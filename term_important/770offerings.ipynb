{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72082939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading quantized model (Salesforce/codet5p-770m)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b1de8b87da4a7aa0328e75681098dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/770 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920fb45252de430a8a1f488a5feb54b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3daa35c05a434bd8a60e4be8044fb8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b762f69d6b804b3795f3033945df1f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098cef7b5f3144e8a5f315be4f0ba851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c4d5826821431f9ce3f0a4714363be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81074e84e02744129e7ff711d99ff241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model & Tokenizer loaded.\n",
      "Trainable params: 7,077,888 / 493,059,072 (1.44%)\n",
      "üìÇ Loading and flattening dataset...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'output_text'],\n",
      "        num_rows: 4200\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_text', 'output_text'],\n",
      "        num_rows: 900\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_text', 'output_text'],\n",
      "        num_rows: 900\n",
      "    })\n",
      "})\n",
      "üß† Tokenizing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1923384d52845d18d65811c027fa25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cde174c97ab479cab8e2ec9d879decb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69aaf10021b401287a09b3cca3bd1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenization complete.\n"
     ]
    }
   ],
   "source": [
    "import os, gc, json, torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from datasets import DatasetDict, Dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PeftModel\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0Ô∏è‚É£  CUDA + Memory Configuration\n",
    "# ------------------------------------------------------------------\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1Ô∏è‚É£  Model + Tokenizer Setup\n",
    "# ------------------------------------------------------------------\n",
    "# CHANGE 1: Updated to 770M model\n",
    "model_name = \"Salesforce/codet5p-770m\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "print(f\"üöÄ Loading quantized model ({model_name})...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    add_bos_token=True,\n",
    "    add_eos_token=True,\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"‚úÖ Model & Tokenizer loaded.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2Ô∏è‚É£  Prepare for LoRA (k-bit training)\n",
    "# ------------------------------------------------------------------\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"k\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.config.use_cache = False\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params, all_param = 0, 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable params: {trainable_params:,} / {all_param:,} \"\n",
    "          f\"({100 * trainable_params / all_param:.2f}%)\")\n",
    "\n",
    "print_trainable_parameters(peft_model)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3Ô∏è‚É£  Dataset Loading\n",
    "# ------------------------------------------------------------------\n",
    "def flatten_jsonl(path):\n",
    "    fixed = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            # Extract prompt and output based on your dataset generator\n",
    "            prompt = obj.get(\"prompt\", \"\")\n",
    "            output = obj.get(\"output\", \"\")\n",
    "\n",
    "            # Ensure strings\n",
    "            if isinstance(prompt, list): prompt = \" \".join(map(str, prompt))\n",
    "            if isinstance(output, list): output = \" \".join(map(str, output))\n",
    "\n",
    "            # Input is just the prompt (no context field in your generator)\n",
    "            input_text = prompt.strip()\n",
    "\n",
    "            fixed.append({\n",
    "                \"input_text\": input_text,\n",
    "                \"output_text\": str(output),\n",
    "            })\n",
    "    return fixed\n",
    "\n",
    "# Update paths to your generated dataset location\n",
    "data_files = {\n",
    "    \"train\": \"/home/sysadm/Music/unitime/unitime_update_dataset/train.jsonl\",\n",
    "    \"validation\": \"/home/sysadm/Music/unitime/unitime_update_dataset/validation.jsonl\",\n",
    "    \"test\": \"/home/sysadm/Music/unitime/unitime_update_dataset/test.jsonl\",\n",
    "\n",
    "}\n",
    "\n",
    "print(\"üìÇ Loading and flattening dataset...\")\n",
    "# Check if files exist before loading to avoid cryptic errors\n",
    "if not os.path.exists(data_files[\"train\"]):\n",
    "    raise FileNotFoundError(\"Run your dataset generator script first to create ./unitime_update_dataset/\")\n",
    "\n",
    "splits = {k: flatten_jsonl(v) for k, v in data_files.items()}\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": Dataset.from_list(splits[\"train\"]),\n",
    "    \"validation\": Dataset.from_list(splits[\"validation\"]),\n",
    "    \"test\": Dataset.from_list(splits[\"test\"]),\n",
    "})\n",
    "print(dataset_dict)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4Ô∏è‚É£  Tokenization\n",
    "# ------------------------------------------------------------------\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 512\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"input_text\"],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        batch[\"output_text\"],\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"üß† Tokenizing...\")\n",
    "tokenized_datasets = dataset_dict.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"input_text\", \"output_text\"],\n",
    ")\n",
    "print(\"‚úÖ Tokenization complete.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5Ô∏è‚É£  Evaluation Metrics\n",
    "# ------------------------------------------------------------------\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple) or (hasattr(preds, \"ndim\") and preds.ndim == 3):\n",
    "        pred_ids = np.argmax(preds, axis=-1)\n",
    "    else:\n",
    "        pred_ids = preds\n",
    "    \n",
    "    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
    "    decoded_preds = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    exact_match = np.mean([p.strip() == l.strip() for p, l in zip(decoded_preds, decoded_labels)])\n",
    "    cer = cer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    decoded_labels_for_bleu = [[label] for label in decoded_labels]\n",
    "    bleu = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels_for_bleu)\n",
    "    \n",
    "    return {\n",
    "        \"exact_match\": round(float(exact_match), 4),\n",
    "        \"cer\": round(float(cer), 4),\n",
    "        \"bleu\": round(float(bleu[\"score\"]), 4),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TrainingArguments configured.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1094849/3017024725.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• Starting fine-tuning Salesforce/codet5p-770m...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/660 1:42:58 < 05:47, 0.10 it/s, Epoch 4.73/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.540800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.260100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.995200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.709100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.419500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.178900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.066800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.007300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 6Ô∏è‚É£  Training Arguments\n",
    "# ------------------------------------------------------------------\n",
    "output_dir = \"./CodeT5p-770m-XML-Tuning\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=5,       # 770m learns faster, 5 epochs might be enough\n",
    "    \n",
    "    # CHANGE 2: Reduced batch size (770m is larger than 220m)\n",
    "    per_device_train_batch_size=2,\n",
    "    \n",
    "    # CHANGE 3: Increased accumulation to maintain effective batch size\n",
    "    gradient_accumulation_steps=16, \n",
    "    \n",
    "    warmup_steps=1,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"no\",       \n",
    "    do_eval=False,\n",
    "    save_strategy=\"steps\",    \n",
    "    save_steps=50,\n",
    "    gradient_checkpointing=True,\n",
    "    load_best_model_at_end=False,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ TrainingArguments configured.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7Ô∏è‚É£  Trainer\n",
    "# ------------------------------------------------------------------\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model, padding=\"longest\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 8Ô∏è‚É£  Training\n",
    "# ------------------------------------------------------------------\n",
    "def pre_train_cleanup():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "pre_train_cleanup()\n",
    "\n",
    "print(f\"\\nüî• Starting fine-tuning {model_name}...\")\n",
    "trainer.train()\n",
    "\n",
    "# SAVE FINAL ADAPTER\n",
    "final_adapter_path = os.path.join(output_dir, \"final_adapter\")\n",
    "trainer.save_model(final_adapter_path)\n",
    "print(f\"üéâ Fine-tuning complete! Adapter saved to '{final_adapter_path}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba7689a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading base model: Salesforce/codet5p-770m\n",
      "üîó Loading adapter from: /home/sysadm/Music/unitime/unitime_nlp/data_generator/CodeT5p-770m-XML-Tuning/final_adapter\n",
      "üìÇ Loading test data...\n",
      "‚úÖ Found 900 test examples.\n",
      "‚ö° Starting generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 225/225 [51:45<00:00, 13.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Computing metrics...\n",
      "\n",
      "==============================\n",
      "   üèÜ EVALUATION RESULTS üèÜ\n",
      "==============================\n",
      "‚úÖ Exact Match: 79.11%\n",
      "üìâ CER:         0.0011  (Lower is better)\n",
      "üìà BLEU:        99.51  (Higher is better)\n",
      "==============================\n",
      "üìù Incorrect predictions saved to 'eval_failures.txt' for debugging.\n"
     ]
    }
   ],
   "source": [
    "import os, json, torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1Ô∏è‚É£ Setup (Must match your training config)\n",
    "# ------------------------------------------------------------------\n",
    "base_model_name = \"Salesforce/codet5p-770m\"\n",
    "adapter_path = \"/home/sysadm/Music/unitime/unitime_nlp/data_generator/CodeT5p-770m-XML-Tuning/final_adapter\" # Path to your saved adapter\n",
    "test_file_path = \"/home/sysadm/Music/unitime/unitime_update_dataset/test.jsonl\"\n",
    "\n",
    "# Load Metric Calculators\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2Ô∏è‚É£ Load Model & Tokenizer\n",
    "# ------------------------------------------------------------------\n",
    "print(f\"üöÄ Loading base model: {base_model_name}\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"üîó Loading adapter from: {adapter_path}\")\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "model.eval() # Set to evaluation mode\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3Ô∏è‚É£ Load & Prepare Test Data\n",
    "# ------------------------------------------------------------------\n",
    "print(\"üìÇ Loading test data...\")\n",
    "input_texts = []\n",
    "reference_texts = []\n",
    "\n",
    "with open(test_file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip(): continue\n",
    "        obj = json.loads(line)\n",
    "        \n",
    "        # Match the flattening logic from training\n",
    "        prompt = obj.get(\"prompt\", \"\")\n",
    "        if isinstance(prompt, list): prompt = \" \".join(map(str, prompt))\n",
    "        input_texts.append(prompt.strip())\n",
    "        \n",
    "        output = obj.get(\"output\", \"\")\n",
    "        if isinstance(output, list): output = \" \".join(map(str, output))\n",
    "        reference_texts.append(output.strip())\n",
    "\n",
    "print(f\"‚úÖ Found {len(input_texts)} test examples.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4Ô∏è‚É£ Generation Loop (Batching for speed)\n",
    "# ------------------------------------------------------------------\n",
    "BATCH_SIZE = 4 # Increase if you have 24GB+ VRAM, decrease if OOM\n",
    "generated_texts = []\n",
    "\n",
    "print(\"‚ö° Starting generation...\")\n",
    "for i in tqdm(range(0, len(input_texts), BATCH_SIZE)):\n",
    "    batch_inputs = input_texts[i : i + BATCH_SIZE]\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        batch_inputs, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            num_beams=1, # greedy search is faster for eval\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    generated_texts.extend(decoded)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5Ô∏è‚É£ Calculate Metrics\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\nüìä Computing metrics...\")\n",
    "\n",
    "# 1. Exact Match (Strict)\n",
    "exact_matches = [1 if gen.strip() == ref.strip() else 0 for gen, ref in zip(generated_texts, reference_texts)]\n",
    "exact_match_score = np.mean(exact_matches) * 100\n",
    "\n",
    "# 2. CER (Character Error Rate) - Lower is better\n",
    "cer_score = cer_metric.compute(predictions=generated_texts, references=reference_texts)\n",
    "\n",
    "# 3. BLEU - Higher is better\n",
    "# BLEU expects references to be a list of lists [[ref1], [ref2]]\n",
    "bleu_refs = [[ref] for ref in reference_texts]\n",
    "bleu_score = bleu_metric.compute(predictions=generated_texts, references=bleu_refs)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"   üèÜ EVALUATION RESULTS üèÜ\")\n",
    "print(\"=\"*30)\n",
    "print(f\"‚úÖ Exact Match: {exact_match_score:.2f}%\")\n",
    "print(f\"üìâ CER:         {cer_score:.4f}  (Lower is better)\")\n",
    "print(f\"üìà BLEU:        {bleu_score['score']:.2f}  (Higher is better)\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Optional: Save failures to inspect\n",
    "with open(\"eval_failures.txt\", \"w\") as f:\n",
    "    for gen, ref in zip(generated_texts, reference_texts):\n",
    "        if gen.strip() != ref.strip():\n",
    "            f.write(f\"EXPECTED:\\n{ref}\\n\\nGOT:\\n{gen}\\n\\n{'='*20}\\n\")\n",
    "print(f\"üìù Incorrect predictions saved to 'eval_failures.txt' for debugging.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6475ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Running Inference Check...\n",
      "Loading adapter from: /home/sysadm/Music/unitime/unitime_nlp/data_generator/CodeT5p-770m-XML-Tuning/final_adapter\n",
      "...Generating XML...\n",
      "\n",
      "--- Generated XML ---\n",
      "<offerings campus=\"woebegon\"\n",
      "           year=\"2010\"\n",
      "           term=\"Fal\"\n",
      "           dateFormat=\"yyyy/M/d\"\n",
      "           timeFormat=\"HHmm\"\n",
      "           created=\"Tue Nov 25 01:14:05 CEST 2025\"\n",
      "           includeExams=\"none\">\n",
      "\n",
      "  <offering offered=\"true\" action=\"insert\">\n",
      "    <course subject=\"DLCS\" courseNbr=\"10\" controlling=\"true\" title=\"Deep Learning\"/>\n",
      "    <config name=\"1\" limit=\"25\">\n",
      "      <subpart type=\"Lab\" suffix=\"\" minPerWeek=\"150\"/>\n",
      "      <class type=\"Lab\" suffix=\"L1\" limit=\"25\"\n",
      "             studentScheduling=\"true\" displayInScheduleBook=\"true\"\n",
      "             cancelled=\"false\" managingDept=\"0100\">\n",
      "        <time days=\"MWF\" startTime=\"0830\" endTime=\"0920\" timePattern=\"3 x 50\"/>\n",
      "        <room building=\"EDUC\" roomNbr=\"106\"/>\n",
      "      </class>\n",
      "    </config>\n",
      "  </offering>\n",
      "</offerings>\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 9Ô∏è‚É£  Inference Check\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\nüîç Running Inference Check...\")\n",
    "# del peft_model, trainer, model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# Load Base Model (770M)\n",
    "model_name = \"Salesforce/codet5p-770m\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load Adapter\n",
    "final_adapter_path=\"/home/sysadm/Music/unitime/unitime_nlp/data_generator/CodeT5p-770m-XML-Tuning/final_adapter\"\n",
    "print(f\"Loading adapter from: {final_adapter_path}\")\n",
    "model = PeftModel.from_pretrained(model, final_adapter_path)\n",
    "model.eval()\n",
    "\n",
    "# Test Input (Matching your generator logic)\n",
    "prompt_text = \"Add a new course offering: DLCS 10 titled 'Deep Learning' as a Lab in EDUC room 106 on MWF 0830-0920 with limit 25.\"\n",
    "input_text = prompt_text.strip()\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(\"...Generating XML...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        num_beams=4,\n",
    "        early_stopping=False,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "xml_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n--- Generated XML ---\")\n",
    "print(xml_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_agentic_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
